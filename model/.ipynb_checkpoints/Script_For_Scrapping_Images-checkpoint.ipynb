{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Code credit:\n",
    "https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
    "Learned from codebasics YouTube Channel\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import requests \n",
    "import io\n",
    "import hashlib\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "def fetch_image_urls_util(url,driver_path):\n",
    "    images = []\n",
    "    # Open main window with URL A\n",
    "    with webdriver.Chrome(executable_path=driver_path) as wd:\n",
    "\n",
    "        # Switch to the new window and open URL B\n",
    "        try:\n",
    "            wd.get(url)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img[class ='irc_mi']\")\n",
    "\n",
    "        for img in thumbnail_results:\n",
    "            if img.get_attribute('src') and 'http' in img.get_attribute('src'):\n",
    "                images.append(img.get_attribute('src'))\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def fetch_image_urls(query:str, max_links_to_fetch:int, wd, sleep_between_interactions:int=1,driver_path= None, target_path = None, search_term = None):\n",
    "    \n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)    \n",
    "    \n",
    "    # build the google query\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "    # load the page\n",
    "    wd.get(search_url.format(q=query))\n",
    "\n",
    "    image_urls = set()\n",
    "    image_count = 0\n",
    "    image_count2 = 0\n",
    "    results_start = 0\n",
    "    i = 0\n",
    "    d = {}\n",
    "    while image_count < max_links_to_fetch:\n",
    "        scroll_to_end(wd)\n",
    "\n",
    "        # get all image thumbnail results\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "        number_results = len(thumbnail_results)\n",
    "        \n",
    "        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n",
    "        \n",
    "        for img in thumbnail_results[50:number_results]:\n",
    "            # try to click every thumbnail such that we can get the real image behind it\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(sleep_between_interactions)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            links = wd.find_elements_by_css_selector(\"a[jsname='sTFXNd']\")\n",
    "\n",
    "            for link in links:\n",
    "                if link.get_attribute('href') and 'http' in link.get_attribute('href'):\n",
    "                    if link.get_attribute('href') not in d:\n",
    "                        d[link.get_attribute('href')] = True\n",
    "                        getactualurl = fetch_image_urls_util(link.get_attribute('href'),driver_path)\n",
    "                    for imageurl in getactualurl:\n",
    "                        if imageurl is not None:\n",
    "                            #print(imageurl)\n",
    "                            image_urls.add(imageurl)\n",
    "            \n",
    "            image_count2 = len(image_urls)\n",
    "            print(image_count2)\n",
    "            if image_count2 >= max_links_to_fetch/10:\n",
    "                print(f\"Found: {len(image_urls)} image links, saving!\")\n",
    "                try:    \n",
    "                    for elem in image_urls:\n",
    "                        persist_image(target_folder,elem)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                image_urls = set()\n",
    "                d = {}\n",
    "\n",
    "            image_count += image_count2\n",
    "                \n",
    "        #image_count = len(image_urls)\n",
    "\n",
    "        if len(image_urls) >= max_links_to_fetch:\n",
    "            print(f\"Found: {len(image_urls)} image links, done!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "            time.sleep(30)\n",
    "            return\n",
    "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "            if load_more_button:\n",
    "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "\n",
    "        # move the result startpoint further down\n",
    "        results_start = image_count\n",
    "\n",
    "    print(len(image_urls))\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "\n",
    "def persist_image(folder_path:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n",
    "        \n",
    "  \n",
    "    \n",
    "def search_and_download(search_term:str,driver_path:str,target_path='./datasets',number_images=50):\n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    with webdriver.Chrome(executable_path=driver_path) as wd:\n",
    "        res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.5,driver_path= driver_path,target_path= target_path,search_term=search_term)\n",
    "    try:    \n",
    "        for elem in res:\n",
    "            persist_image(target_folder,elem)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "import time\n",
    "import requests \n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "import hashlib\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "query = [\"Serena Williams\"]\n",
    "\n",
    "for q in query:\n",
    "    search_and_download(q,\"./chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
